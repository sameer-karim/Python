{\rtf1\ansi\ansicpg1252\cocoartf2638
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ### Steps\
#### 1- Import necessary libraries\
#### 2- Read the dataset\
#### 3- Explore the data\
#### 4- Feature selection\
#### 5- Build the model\
#### 6- Evaluate the model\
#### 7- Save the model\
pip install numpy pandas sklearn xgboost\
#### 1- Import necessary libraries \
import os, sys\
import numpy as np\
import pandas as pd\
from xgboost import XGBClassifier\
from sklearn.model_selection import train_test_split\
from sklearn.preprocessing import MinMaxScaler\
from sklearn.metrics import accuracy_score\
import sklearn.metrics as metrics\
import matplotlib.pyplot as plt\
import seaborn as sns\
import warnings\
warnings.filterwarnings('ignore')\
from sklearn.preprocessing import LabelEncoder\
from sklearn.feature_selection import SelectKBest\
from sklearn.feature_selection import chi2\
import joblib\
\
#### 2- Read the dataset\
parkinson_df= pd.read_csv('parkinsons.csv')\
pd.set_option("display.max_columns", None)\
\
parkinson_df.head(5)\
#### 3- Explore data\
parkinson_df.shape\
parkinson_df.info()\
parkinson_df.describe()\
parkinson_df.corr()\
### Dataset attributes \
#### \'95 MDVP:Fo(Hz) - Average vocal fundamental frequency\
#### \'95 MDVP:Fhi(Hz) - Maximum vocal fundamental frequency\
#### \'95 MDVP:Flo(Hz) - Minimum vocal fundamental frequency\
#### \'95 MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several measures of variation in fundamental frequency\
#### \'95 MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude\
#### \'95 NHR,HNR - Two measures of ratio of noise to tonal components in the voice\
#### \'95 status - Health status of the subject (one) - Parkinson's, (zero) - healthy\
#### \'95 RPDE,D2 - Two nonlinear dynamical complexity measures\
#### \'95 DFA - Signal fractal scaling exponent\
#### \'95 spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation\
#### Exploring features\
\
sns.countplot(parkinson_df['status'])\
#### The previous plot shows that more than 140 individuals suffers from Parkinson, while the rest 50 individuals don't \
\
fig, ax = plt.subplots(figsize=(12, 8))\
corr = parkinson_df.corr()\
ax = sns.heatmap(corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=200))\
#### 4- Feature Selection\
#Rearrange the columns\
parkinson_df = parkinson_df[["name", "MDVP:Fo(Hz)", "MDVP:Fhi(Hz)", "MDVP:Flo(Hz)", "MDVP:Jitter(%)", "MDVP:Jitter(Abs)", "MDVP:RAP", "MDVP:PPQ", "Jitter:DDP", "MDVP:Shimmer", "MDVP:Shimmer(dB)", "Shimmer:APQ3", "Shimmer:APQ5", "MDVP:APQ", "Shimmer:DDA", "NHR", "HNR", "RPDE", "DFA", "spread1", "spread2", "D2", "PPE", "status"]]\
\
\
#Create a copy of the original dataset\
df2= parkinson_df.copy()\
\
#Assign numeric values to the binary and categorical columns\
number= LabelEncoder()\
df2['name']= number.fit_transform(df2['name'])\
\
df2.head(5)\
X= df2.iloc[:,0:11] #all features \
Y= df2.iloc[:,-1] #target (status of Parkinson)\
\
best_features= SelectKBest(score_func=chi2, k=3) #function that select the top 3 features.\
fit= best_features.fit(X,Y) \
\
#Creating dataframes for the features and the score of each feature.\
Parkinson_scores= pd.DataFrame(fit.scores_)\
Parkinson_columns= pd.DataFrame(X.columns)\
\
#Create a dataframe that combines all the features and their corresponding scores.\
features_scores= pd.concat([Parkinson_scores, Parkinson_scores], axis=1)\
features_scores.columns= ['Features', 'Score']\
features_scores.sort_values(by = 'Score')\
#### From the correlation heatmap and feature selection step we conclude that the 3 most affecting features on the target out put are:\
#### 1- MDVP:Flo(Hz)\
#### 2- MDVP:Fo(Hz)\
#### 3- MDVP:Fhi(Hz)\
#### 5- Build the model\
x= parkinson_df[["MDVP:Flo(Hz)", "MDVP:Fo(Hz)", "MDVP:Fhi(Hz)"]]\
y= parkinson_df[["status"]]\
x_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.2, random_state=7)\
\
model=XGBClassifier()\
model.fit(x_train,y_train)\
#### 6- Evaluate the model\
y_pred=model.predict(x_test)\
print(accuracy_score(y_test, y_pred)*100)\
#define metrics\
y_pred_proba= model.predict_proba(x_test) [::,1]\
\
#Calculate true positive and false positive rates\
false_positive_rate, true_positive_rate, _ = metrics.roc_curve(y_test, y_pred_proba)\
\
#Calculate the area under curve to see the model performance\
auc= metrics.roc_auc_score(y_test, y_pred_proba)\
\
#Create ROC curve\
plt.plot(false_positive_rate, true_positive_rate,label="AUC="+str(auc))\
plt.title('ROC Curve')\
plt.ylabel('True Positive Rate')\
plt.xlabel('false Positive Rate')\
plt.legend(loc=4)\
#### The area under the curve (AUC) is 0.84, which is very close to one, meaning that the model did a good job.\
\
#### 7- Save the model\
# Save the trained model to a file to be used in future predictions \
joblib.dump(model, 'XG.pkl')\
\
}